encode_max_length: 100
eval_decode_max_length: 30
truncation: True
model_type: 'bart_mask_random'
pretrained_model_path: '../init/init_model/bart-base'
data_dir: '../data/negg_data'
annotation: 'bart_base_contrastive_fine-tuning'
checkpoint : '../cache/checkpoints/02-09/bart_base_event-centric_pretraining/epoch32/best_checkpoint.pt' # TODO
resume: True
use_gpu: True
multi_gpu: False
pro_type : 'sqrt'
gpuid: '3'
gpu_num: 1
train: True
eval: False
test: True
filemode: 'w'
patience : 5
num_train_epochs: 100
max_train_steps: 0
per_gpu_train_batch_size: 64
gradient_accumulation_steps: 1
eval_batch_size: 64
log_step: 10
margin: 0.5
noise_lambda: 0
denominator_correction_factor : 0
pretrain: False
random_span: False
softmax: True
lr_scheduler_type: 'constant' # constant cosine
loss_fct: 'ComplementEntropy' # CrossEntropyLoss MarginRankingLoss 
beta : 1
fp16: False
lr: 1.0e-5
weight_decay: 1.0e-6  # 1.0e-6 #1e-2
epsilon: 1.0e-8
vocab_size: 50265
seed: 2022

dynamic_weight: True # new add 
pred_order: False # new add 